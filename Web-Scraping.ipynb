{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "1) virtual-youtuber.userlocal.jp - Website that stores a massive list of existing VTubers\n",
    "\n",
    "2) youtube.com - world famous video hosting\n",
    "\n",
    "3) twitch.tv - world famous streaming service\n",
    "\n",
    "4) playboard.co - collectoin of the data for YouTube channels worldwide to determine and announce reliable rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "\n",
    "I want to build a graph of time and number of appearing vtubers. I will get needed data by using selenium. Data about existing Vtubers is obtained from virtual-youtuber.userlocal.jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#web-site url assignment\n",
    "url = 'https://virtual-youtuber.userlocal.jp/document/ranking?page='\n",
    "\n",
    "#creating puppet browser\n",
    "browser = webdriver.Firefox()\n",
    "\n",
    "#create dictionary of lists to write down parsed vtuber names and subscribers' count\n",
    "vtuber = {}\n",
    "vtuber['name'] = []\n",
    "vtuber['subs'] = []\n",
    "vtuber['registration date'] = []\n",
    "\n",
    "#list of vtubers has 40 pages, so go through them and concatenate page number with site's url\n",
    "for i in range(1, 41):\n",
    "    browser.get(url + str(i))\n",
    "    time.sleep(2)#wait 2 seconds to let web-site load\n",
    "    \n",
    "    #try-except block that finds html elements that I need\n",
    "    try:\n",
    "        name = browser.find_elements_by_class_name('no-propagation')\n",
    "        subs = browser.find_elements_by_class_name('text-success')\n",
    "        name = name[1::3]#to delete unneeded elements\n",
    "    \n",
    "    except:\n",
    "        print('An Exception has occurred.')\n",
    "    \n",
    "    #assign obtained values to appropriate lists of dictionary\n",
    "    for i in range(len(name)):\n",
    "        vtuber['name'].append(name[i].text)\n",
    "        vtuber['subs'].append(subs[i].text)\n",
    "\n",
    "#close browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's obtain data about registration date of each VTuber in the set. Luckily for us, youtube.com does provide desired information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Scraping took about 10 hours due to amount of query and delay between them\n",
    "#Create puppet browser and make it access youtube.com\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('https://www.youtube.com')\n",
    "\n",
    "\n",
    "#go through every vtuber in the set\n",
    "for i in range(len(vtuber['name'])):\n",
    "    \n",
    "    #let web-site load its objects\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        #find searchbox, write down vtuber name and add 'ch' to be sure that first link to output will be desired channel\n",
    "        searchbox = browser.find_element_by_tag_name('input')\n",
    "        searchbox.send_keys(vtuber['name'][i])\n",
    "        searchbox.submit()\n",
    "        \n",
    "        #Let the page download, find filter button and filter outputs so that they show only channels\n",
    "        time.sleep(6)\n",
    "        filter_dropdown = browser.find_element_by_xpath('/html/body/ytd-app/div/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[1]/div[2]/ytd-search-sub-menu-renderer/div[1]/div/ytd-toggle-button-renderer/a')\n",
    "        filter_dropdown.click()\n",
    "        filter_channels = browser.find_element_by_xpath('/html/body/ytd-app/div/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[1]/div[2]/ytd-search-sub-menu-renderer/div[1]/iron-collapse/div/ytd-search-filter-group-renderer[2]/ytd-search-filter-renderer[2]/a/div/yt-formatted-string')\n",
    "        filter_channels.click()\n",
    "        \n",
    "        #wait for 2 seconds to let the page load\n",
    "        time.sleep(2)\n",
    "        #find channel link and click it\n",
    "        channel_link = browser.find_element_by_xpath('//*[@id=\"main-link\"]')\n",
    "        channel_link.click()\n",
    "        \n",
    "        #wait for a while to let page load, go to channel description and get registration data\n",
    "        time.sleep(2)\n",
    "        channel_description = browser.find_element_by_xpath('/html/body/ytd-app/div/ytd-page-manager/ytd-browse/div[3]/ytd-c4-tabbed-header-renderer/tp-yt-app-header-layout/div/tp-yt-app-header/div[2]/tp-yt-app-toolbar/div/div/tp-yt-paper-tabs/div/div/tp-yt-paper-tab[6]/div')\n",
    "        channel_description.click()\n",
    "        \n",
    "        \n",
    "        registr_date = browser.find_element_by_xpath('/html/body/ytd-app/div/ytd-page-manager/ytd-browse/ytd-two-column-browse-results-renderer/div[1]/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-channel-about-metadata-renderer/div[2]/yt-formatted-string[2]/span[2]')\n",
    "        \n",
    "        #Clear searchbox to start again\n",
    "        browser.find_element_by_tag_name('input').send_keys(Keys.CONTROL, 'a')\n",
    "        browser.find_element_by_tag_name('input').send_keys(Keys.DELETE)\n",
    "        \n",
    "    #handle exceptions by skipping null elements    \n",
    "    except (exceptions.StaleElementReferenceException, exceptions.NoSuchElementException) as e:\n",
    "        print(e)\n",
    "        browser.find_element_by_tag_name('input').send_keys(Keys.CONTROL, 'a')\n",
    "        browser.find_element_by_tag_name('input').send_keys(Keys.DELETE)\n",
    "        vtuber['registration date'].append('null')\n",
    "            \n",
    "        continue\n",
    "    \n",
    "    except (exceptions.WebDriverException, exceptions.ElementNotInteractableException) as e:\n",
    "        print(e)\n",
    "        browser.find_element_by_tag_name('input').send_keys(Keys.CONTROL, 'a')\n",
    "        browser.find_element_by_tag_name('input').send_keys(Keys.DELETE)\n",
    "        vtuber['registration date'].append('null')\n",
    "        continue\n",
    "        \n",
    "    #add it to our dictionary of lists\n",
    "    vtuber['registration date'].append(registr_date.text)\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formate dates: split them by quarters of years\n",
    "\n",
    "q1 = ['янв', 'фев', 'мар']\n",
    "q2 = ['апр', 'мая', 'июн']\n",
    "q3 = ['июл', 'авг', 'сен']\n",
    "q4 = ['окт', 'ноя', 'дек']\n",
    "\n",
    "for i in range(len(vtuber['registration date'])):\n",
    "    for j in range(3):\n",
    "        if q1[j] in vtuber['registration date'][i]:\n",
    "            new_date = vtuber['registration date'][i][:-3].split()\n",
    "            new_date[0:2] = 'Q1, '\n",
    "            vtuber['registration date'][i] = ''.join(new_date)\n",
    "            \n",
    "        elif q2[j] in vtuber['registration date'][i]:\n",
    "            new_date = vtuber['registration date'][i][:-3].split()\n",
    "            new_date[0:2] = 'Q2, '\n",
    "            vtuber['registration date'][i] = ''.join(new_date)\n",
    "            \n",
    "        elif q3[j] in vtuber['registration date'][i]:\n",
    "            new_date = vtuber['registration date'][i][:-3].split()\n",
    "            new_date[0:2] = 'Q3, '\n",
    "            vtuber['registration date'][i] = ''.join(new_date)\n",
    "            \n",
    "        elif q4[j] in vtuber['registration date'][i]:\n",
    "            new_date = vtuber['registration date'][i][:-3].split()\n",
    "            new_date[0:2] = 'Q4, '\n",
    "            vtuber['registration date'][i] = ''.join(new_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now write obtained data to the csv file\n",
    "fieldnames = ['name', 'subs', 'registration date']\n",
    "\n",
    "with open('Datasets/vtubers_date.csv', 'w', encoding = 'utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for i in range(len(vtuber['name'])):\n",
    "        w.writerow({'name': vtuber['name'][i], 'subs': vtuber['subs'][i], 'registration date': vtuber['registration date'][i]})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Dataset from kaggle contains only data from youtube, but VShojo agency actually is a twitch based agency and that's why their members' stats are different from the truth. Here I am going to fix it and obtain info about them from twitch.tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Projekt Melody',\n",
       " 'ironmouse',\n",
       " 'Silvervale',\n",
       " 'Nyanners',\n",
       " 'HimeHajime',\n",
       " 'Veibae',\n",
       " 'Apricot',\n",
       " 'Zentreya']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take only those rows from dataframe, whose affiliation is VShojo\n",
    "df = pd.read_csv('Datasets/channels.csv')\n",
    "df = df[df.affiliation == 'VShojo']\n",
    "\n",
    "#Sort it by deleting archive channels\n",
    "vshojo_names = list(df[~df.name.str.contains('Archive')]['name'])\n",
    "vshojo_names = [x for x in vshojo_names if 'VOD' not in x]\n",
    "vshojo_names = [x for x in vshojo_names if 'VShojo' not in x]\n",
    "vshojo_names[-2] = 'Apricot'\n",
    "\n",
    "vshojo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': ['Projekt Melody', 'ironmouse', 'Silvervale', 'Nyanners', 'HimeHajime', 'Veibae', 'Apricot', 'Zentreya'], 'subs': ['512,5 тыс. фолловеров', '825,1 тыс. фолловеров', '237,8 тыс. фолловеров', '690,8 тыс. фолловера', '103,4 тыс. фолловеров', '626,4 тыс. фолловера', '272,2 тыс. фолловеров', '247,5 тыс. фолловеров']}\n"
     ]
    }
   ],
   "source": [
    "#Create dictionaries. One to store VShojo members' data and another to store html objects' addresses\n",
    "vshojo = {}\n",
    "vshojo['name'] = []\n",
    "vshojo['subs'] = []\n",
    "\n",
    "html_classes = {'searchbox': 'btlhBn', 'button': 'fSetzA', 'link': 'fcQsXy',\n",
    "           'xpath': '/html/body/div[1]/div/div[2]/div[1]/main/div[2]/div[3]/div/div/div[1]/div[1]/div[2]/div/div[2]/div[1]/div[1]/div[2]/p'}\n",
    "\n",
    "#Create puppet browser, open twitch and wait for 5 seconds to let it load\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('https://www.twitch.tv')\n",
    "time.sleep(5)\n",
    "\n",
    "#go through every VShojo member\n",
    "for name in vshojo_names:\n",
    "    try:\n",
    "        #Find Searchbox, write down member's name and click find button to activate search engine\n",
    "        searchbox = browser.find_element_by_class_name(html_classes['searchbox'])\n",
    "        button = browser.find_element_by_class_name(html_classes['button'])\n",
    "        searchbox.send_keys(name)\n",
    "        button.click()\n",
    "        \n",
    "        #let a new page load and find link, leading to channel description\n",
    "        time.sleep(2)\n",
    "        link = browser.find_elements_by_class_name(html_classes['link'])\n",
    "        link[0].click()\n",
    "        \n",
    "        #let a new page load and finally obtain channel information\n",
    "        time.sleep(5)\n",
    "        follower_count = browser.find_element_by_xpath(html_classes['xpath'])        \n",
    "    except:\n",
    "        print('Was not able to find an element with that name.')\n",
    "\n",
    "    #add data to dictionary\n",
    "    vshojo['name'].append(name)\n",
    "    vshojo['subs'].append(follower_count.text)\n",
    "\n",
    "#close browser and output obtained data(just for curiosity)\n",
    "browser.quit()\n",
    "print(vshojo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to a csv file, so that we could use it in our project\n",
    "fieldnames = ['name', 'subs']\n",
    "\n",
    "with open('Datasets/vshojo.csv', 'w', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames = fieldnames)\n",
    "    w.writeheader()\n",
    "    for i in range(len(vshojo['name'])):\n",
    "        w.writerow({'name': vshojo['name'][i], 'subs': vshojo['subs'][i]})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "I want to get a tiny amount of data just to show the most profitable channels ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = {}\n",
    "top['name'] = []\n",
    "top['revenue'] = []\n",
    "\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('https://playboard.co/en/youtube-ranking/most-superchatted-all-channels-in-worldwide-total')\n",
    "try:\n",
    "    revenue = browser.find_elements_by_class_name('score')\n",
    "    name = browser.find_elements_by_tag_name('h3')\n",
    "except:\n",
    "    print('An exception has occured')\n",
    "\n",
    "revenue = revenue[2::4]\n",
    "name = name[5:]\n",
    "for i in range(len(revenue)):\n",
    "    top['name'].append(name[i].text)\n",
    "    top['revenue'].append(revenue[i].text)\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = ['name', 'revenue']\n",
    "\n",
    "with open('Datasets/top_revenue.csv', 'w', encoding = 'utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for i in range(len(top['name'])):\n",
    "        w.writerow({'name': top['name'][i], 'revenue': top['revenue'][i]})\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
